略称,Year,Paper URL,Website URL,Challenge Tag,Sub-Challeng Tag,How to Solve,Training Type,Dataset,Evaluation
VLA-Example,2024,https://example.com/paper.pdf,https://example.com,Multi-Modal Fusion and Physical World Representation; From Complex Instructions to Robust and Real-Time Execution; Generalization and Adaptation for Continuous Learning; Security Interpretability and Reliable Interaction;Dataset Construction and Benchmarking Standards,sub-challeng-1;sub-challeng-2,Methods,BC,Example Dataset1;Example Dataset2,Example Bechmark1;Example Bechmark2
OTTER,2021,https://arxiv.org/pdf/2503.03734,https://ottervla.github.io/,Multi-Modal Fusion and Physical World Representation,The GAP between Sematics Perception and Physical Interation,introdues a text-aware feature extraction which preserves semantics aligned with task description,BC,MIMIC-IT,POPE;MSRVTT-QA
LIV,2023,https://proceedings.mlr.press/v202/ma23b/ma23b.pdf,https://penn-pal-lab.github.io/LIV/,Multi-Modal Fusion and Physical World Representation,The GAP between Sematics Perception and Physical Interation,introdues a contrastive framework on robot-control data to construct a joint vision Clanguage embeddingspace,RL,Hopper-v2; HalfCheetah-v2; Walker2d-v2,MuJoCo
CoT-VLA,2025,https://www.alphaxiv.org/pdf/2407.03982,https://cot-vla.github.io/,From Complex Instructions to Robust Real-Time Execution Hierarchical Planning and Task Decomposition,Employs pixel-level subgoal images as explicit intermediates,BC,CalVIN ;RLBench,RLBench,-
OTTER,2021,https://arxiv.org/pdf/2503.03734?,https://ottervla.github.io/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Introdues a text-aware feature extraction which preserves semantics aligned with task description,BC,MIMIC-IT (MultI-Modal In-Context Instruction Tuning),POPE; MSRVTT-QA
LIV,2023,https://proceedings.mlr.press/v202/ma23b/ma23b.pdf,https://penn-pal-lab.github.io/LIV/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Introdues a contrastive framework on robot-control data to construct a joint vision Clanguage embeddingspace,RL,Hopper-v2; HalfCheetah-v2; Walker2d-v2,MuJoCo Environment
ACT-LLM,2025,https://arxiv.org/pdf/2506.21250,https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Formulates raw per- ceptual inputs into structured language representation,RL,BridgeData; OXE,-
Look-Leap,2023,https://arxiv.org/pdf/2311.17842,robot-vila.github.io,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Structured action-plan generation from visual inputs,Predictive Modeling,null,Real-Robot Benchmark
RT-2,2023,https://arxiv.org/pdf/2307.12307,https://deepai.org/publication/robust-weighted-sum-rate-maximization-for-transmissive-ris-transmitter-enabled-rsma-networks,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Fine-tuning a pretrained VLM to directly output action tokens,BC,RoboCat,Real-Robot Benchmark
Prompt-a-Robot-to-Walk,2024,https://arxiv.org/pdf/2307.12307,https://deepai.org/publication/robust-weighted-sum-rate-maximization-for-transmissive-ris-transmitter-enabled-rsma-networks,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Fine-tuning a pretrained VLM to directly output action tokens,BC and Predictive Modeling,OXE; RT-X; RoboNet,Real-Robot Benchmark
Grounding MLLMs,2024,https://arxiv.org/pdf/2406.07904,https://github.com/mbzuai-oryx/groundingLMM,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Fine-tuning a pretrained VLM to directly output action tokens,BC,RT-X and RL Bench,Real-Robot Benchmark
OpenVLA,2024,https://arxiv.org/pdf/2406.09246,https://openvla.github.io/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Fine-tuning a pretrained VLM to directly output action tokens,BC,OXE; RT-X; RoboNet,Bridge Data
CLIP-RT,2024,https://arxiv.org/pdf/2411.00508,https://clip-rt.github.io,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Extends CLIP-style vision Clanguage alignment,BC,RT-X,RT-X Bechmark
Humanoid-VLA,2025,https://arxiv.org/pdf/2502.14795,https://github.com/AllenXuuu/HumanVLA,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Pre-training language-action,BC and Predictive Modeling and RL,Human Video Datasets; Robot Manipalation Datasets; Multimodal and Multitask Datasets,Whole-Body Locomotion and Manipulation Task Benchmarks
VoxPoser,2023,https://arxiv.org/pdf/2307.05973,https://voxposer.github.io/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Generate intermediate programs and 3D affordance maps as strong intermediate representations by LLM,BC,Mini-RLBench; ALOHA (A Lot of HAnds) ;Bridge-Data-V2,RLBench (Robot Learning Benchmark)
Orion,2025,https://arxiv.org/pdf/2503.19755,https://xiaomi-mlab.github.io/Orion/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Introduces a high-level VLM planner with a separate low-level motion controller,BC,OCTO (Open-source Collaborative Training Objects); Bridge-V2,RT-X benchmarks
Gemini RObotics,2025,https://arxiv.org/pdf/2503.20020,https://deepmind.google/blog/gemini-robotics-brings-ai-into-the-physical-world/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Introduces a high-level VLM planner with a separate low-level motion controller,BC and Predictive Modeling,BridgeDat V2; RH20T; AgiBot World Colosseo,Manipulation Tasks
KnowledgeVLA,2025,https://arxiv.org/pdf/2104.09044,https://github.com/dvlab-research/ReviewKD,"Multi-Modal Fusionand Physical World Representation",The GAP between Sematics Perception and Physical Interation,Introduces a high-level VLM planner with a separate low-level motion controller,BC,BridgeDat V2,Closed-loop simulation platform
Beyond Sight,2025,https://arxiv.org/pdf/2501.04693,https://fuse-model.github.io/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Incorporating additional modalities,BC,OXE,Bridge Data
RH-20T,2023,https://arxiv.org/pdf/2307.00595,https://rh20t.github.io./,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Incorporating additional modalities,BC,Rh20t,Rh20t
TouchVLA,2025,https://arxiv.org/pdf/2507.17294,https://clear-nus.github.io/blog/vla-touch,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Incorporating additional modalities,BC,TouchVLA,TouchVLA
TLA,2025,https://arxiv.org/pdf/2503.08548,https://sites.google.com/view/tactile-language-action/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Introduces tactile perception,BC,TLA,Custom Contact-Rich Manipulation Benchmark
OmniVTLA,2025,https://arxiv.org/pdf/2508.08706,https://github.com/linchangyi1/Awesome-Touch,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Constructs a se mantically aligned tactile encoder,BC,OmniVTLA,Real-Robot Benchmark
Tactile-VLA,2025,https://arxiv.org/pdf/2507.09160,https://jialeihuang.github.io/tactileVLA.github.io/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Ranging from deep fusion across the full pipeline,BC,Touch-VLA,Real-Robot Benchmark
ForceVLA,2025,https://arxiv.org/pdf/2505.22159,https://sites.google.com/view/forcevla2025/,"Multi-Modal Fusion and Physical World Representation",The GAP between Sematics Perception and Physical Interation,Modular mixture-of-experts (MoE) fusion,BC,Force-VLA-Dataset,Force-VLA-Benchmark
