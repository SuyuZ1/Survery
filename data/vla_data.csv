Abbreviation,Year,Link,Project Website,Challenge Tag,Sub-Challeng Tag,How to Solve,Training Type,Dataset,Evaluation
OTTER,2021,https://arxiv.org/pdf/2503.03734?,https://ottervla.github.io/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,introdues a text-aware feature extraction which preserves semantics aligned with task description,BC,MIMIC-IT (MultI-Modal In-Context Instruction Tuning),POPE; MSRVTT-QA
LIV,2023,https://proceedings.mlr.press/v202/ma23b/ma23b.pdf,https://penn-pal-lab.github.io/LIV/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,introdues a contrastive framework on robot-control data to construct a joint vision¨Clanguage embeddingspace,RL,Hopper-v2; HalfCheetah-v2; Walker2d-v2,?MuJoCo Environment
ACT-LLM,2025,https://arxiv.org/pdf/2506.21250,https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,formulates raw per- ceptual inputs into structured language representation,RL,BridgeData; OXE,
Look-Leap,2023,https://arxiv.org/pdf/2311.17842,robot-vila.github.io,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"translates visual information not just into a state
description; but into a complete; structured action plan",Predictive Modeling,GPT-4V as a pre-trained visual language model,Real-Robot Benchmark
RT-2,2023,https://arxiv.org/pdf/2307.12307,https://deepai.org/publication/robust-weighted-sum-rate-maximization-for-transmissive-ris-transmitter-enabled-rsma-networks,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,pretrained VLM can be fine-tuned to directly output robot actions.,BC,RoboCat,Real-Robot Benchmark
Prompt-a-Robot-to-Walk,2024,https://arxiv.org/pdf/2307.12307,https://deepai.org/publication/robust-weighted-sum-rate-maximization-for-transmissive-ris-transmitter-enabled-rsma-networks,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,pretrained VLM can be fine-tuned to directly output robot actions.,BC and Predictive Modeling,OXE; RT-X; RoboNet,Real-Robot Benchmark
Grounding MLLMs,2024,https://arxiv.org/pdf/2406.07904,https://github.com/mbzuai-oryx/groundingLMM,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,pretrained VLM can be fine-tuned to directly output robot actions.,BC,RT-X and RL Bench,Real-Robot Benchmark
OpenVLA,2024,https://arxiv.org/pdf/2406.09246,https://openvla.github.io/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,pretrained VLM can be fine-tuned to directly output robot actions.,BC,OXE; RT-X; RoboNet,Bridge Data
CLIP-RT,2024,https://arxiv.org/pdf/2411.00508,https://clip-rt.github.io,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,extends CLIP¡¯s vision¨Clanguage alignment concept to generate action representations conditioned on visual         scenes and task instructions,BC,RT-X,RT-X Bechmark
Humanoid-VLA,2025,https://arxiv.org/pdf/2502.14795,https://github.com/AllenXuuu/HumanVLA,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"language-action pre
training to reduce the gap between semantic and motor rep
resentations",BC and Predictive Modeling and RL,Human Video Datasets; Robot Manipalation Datasets; Multimodal and Multitask Datasets,Whole-Body Locomotion and Manipulation Task Benchmarks
VoxPoser,2023,https://arxiv.org/pdf/2307.05973,https://voxposer.github.io/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"leverages an LLM¡¯s reasoning
ability to generate intermediate programs and 3D affordance
maps as strong intermediate representations for grounding
visual¨Clinguistic understanding into spatial actions",BC,Mini-RLBench; ALOHA (A Lot of HAnds) ;Bridge-Data-V2,RLBench (Robot Learning Benchmark)
Orion,2025,https://arxiv.org/pdf/2503.19755,https://xiaomi-mlab.github.io/Orion/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"VLM acts as a high-level planner for se
mantic reasoning; while a separate low-level policy handles
high-frequency motion execution",BC,OCTO (Open-source Collaborative Training Objects); Bridge-V2,RT-X benchmarks
Gemini RObotics,2025,https://arxiv.org/pdf/2503.20020,https://deepmind.google/blog/gemini-robotics-brings-ai-into-the-physical-world/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"VLM acts as a high-level planner for se
mantic reasoning; while a separate low-level policy handles
high-frequency motion execution",BC and Predictive Modeling,BridgeDat V2; RH20T; AgiBot World Colosseo,Manipulation Tasks
KnowledgeVLA,2025,https://arxiv.org/pdf/2104.09044,https://github.com/dvlab-research/ReviewKD,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"VLM acts as a high-level planner for se
mantic reasoning; while a separate low-level policy handles
high-frequency motion execution",BC,BridgeDat V2,Closed-loop simulation platform
Beyond Sight,2025,https://arxiv.org/pdf/2501.04693,https://fuse-model.github.io/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"incorporating addi
tional modalities; such as tactile; force; and audio sensing; is
an inevitable trend for achieving more comprehensive and
reliable perception",BC,OXE,Bridge Data
RH-20T,2023,https://arxiv.org/pdf/2307.00595,https://rh20t.github.io./,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"incorporating addi
tional modalities; such as tactile; force; and audio sensing; is
an inevitable trend for achieving more comprehensive and
reliable perception",BC,Rh20t,Rh20t
TouchVLA,2025,https://arxiv.org/pdf/2507.17294,https://clear-nus.github.io/blog/vla-touch,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"incorporating addi
tional modalities; such as tactile; force; and audio sensing; is
an inevitable trend for achieving more comprehensive and
reliable perception",BC,TouchVLA,TouchVLA
TLA,2025,https://arxiv.org/pdf/2503.08548,https://sites.google.com/view/tactile-language-action/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"introduces tactile perception to enhance accuracy in contact
rich manipulation",BC,TLA,Custom Contact-Rich Manipulation Benchmark
OmniVTLA,2025,https://arxiv.org/pdf/2508.08706,https://github.com/linchangyi1/Awesome-Touch,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"constructs a se
mantically aligned tactile encoder to bridge tactile feedback
and linguistic concepts in the feature space.",BC,?OmniVTLA,Real-Robot Benchmark
Tactile-VLA,2025,https://arxiv.org/pdf/2507.09160,https://jialeihuang.github.io/tactileVLA.github.io/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"ranging from deep fusion across
the entire decision-making pipeline in Tactile-VLA",BC,Touch-VLA?,Real-Robot Benchmark
ForceVLA,2025,https://arxiv.org/pdf/2505.22159,https://sites.google.com/view/forcevla2025/,"Multi-Modal Fusion
and Physical World
Representation",The GAP between Sematics  Perception  and Physical Interation,"to modular mixture-of-experts (MoE) fusion that preserves
original VLM representations",BC,Force-VLA-Dataset,Force-VLA-Benchmark

