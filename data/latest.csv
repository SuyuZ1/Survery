略称,Year,Paper URL,Website URL,Challenge Tag,Sub-Challeng Tag,How to Solve,Training Type,Dataset,Evaluation
ProphRL, 2025-11-25, https://arxiv.org/pdf/2511.20633, https://LogosRoboticsGroup.github.io/ProphRL,Generalization and Adaptation for Continuous Learning,Online Interaction and Reinforcement Learning,Pretrain a unified action-conditioned world model (Prophet) on diverse robot data,BC and Predictive Modeling and RL, AgiBot;DROID;OXE;LIBERO,LIBERO;SimplerEnv-WidowX;BRIDGE
DiG-Flow, 2025-12-1, https://arxiv.org/pdf/2512.01715, https://beingbeyond.github.io/DiG-Flow,Generalization and Adaptation for Continuous Learning,Sim-to-real Gap in Deployment,Compute a distributional discrepancy between observation and action embeddings to guide residual feature updates for robust flow matching,Predictive Modeling,LIBERO;RoboCasa,LIBERO;RoboCasa
RAGNet, 2025-7-31, https://arxiv.org/pdf/2507.23734, https://github.com/Dexmal-AI/RAGNet, Dataset Construction and Benchmarking Standards, Evaluation and Benchmark, construct a large-scale reasoning-based affordance segmentation dataset and propose AffordanceNet for open-world grasping, -, HANDAL;OXE;EgoObjects;GraspNet, HANDAL;GraspNet Novel;3DOI;RLBench
VLA-4D, 2025-11-21, https://arxiv.org/pdf/2511.17199, -, Multi-Modal Fusion and Physical World Representation, From 2D Images to SPatial Temporal Representations, Embed 3D positions and 1D time into visual features and extend actions with temporal variables for coherent manipulation, Predictive Modeling, Scan2Cap;ScanQA;ScanRef;LIBERO, LIBERO
LatBot, 2025-11-28, https://arxiv.org/pdf/2511.23034, https://mm-robot.github.io/distill_latent_action, Multi-Modal Fusion and Physical World Representation, Dynamic and Predictive World Models, Learn instruction-guided latent actions from multi-frame videos and jointly decode future frames and inter-frame actions, BC and Predictive Modeling, OXE;AgiBot;EgoDex;DROID, SIMPLER;LIBERO;Franka real-robot tasks
DeepThinkVLA, 2025-10-31, https://arxiv.org/pdf/2511.15669, https://github.com/wadeKeith/DeepThinkVLA, From Complex Instructions to Robust and Real-Time Execution, Real-Time Execution and Computing Efficiency, Use a hybrid-attention decoder with SFT then outcome-based RL to align reasoning with actions, BC and RL, LIBERO demonstrations;constructed embodied CoT dataset, LIBERO
Mantis, 2025-11-20, https://arxiv.org/pdf/2511.16175, https://github.com/zhijie-group/Mantis, Multi-Modal Fusion and Physical World Representation, Dynamic and Predictive World Models, Disentangle visual foresight from action learning using a diffusion transformer head with latent action queries and progressive multimodal training, BC and Predictive Modeling, Something-Something V2;DROID;LLaVA-Instruct and 38 multimodal datasets, LIBERO
X-VLA, 2025-10-11, https://arxiv.org/pdf/2510.10274, https://thu-air-dream.github.io/X-VLA/, Generalization and Adaptation for Continuous Learning, Open-World Generalization, Learn embodiment-specific soft prompts to absorb cross-embodiment heterogeneity and enable scalable pretraining and efficient adaptation, BC and Predictive Modeling, AGIBOT-beta;Droid;RoboMind;Soft-FOLD, LIBERO;Simpler-WidowX;Calvin;RoboTwin-2.0;VLABench
MergeVLA, 2025-11-24, https://arxiv.org/pdf/2511.18810, https://mergevla.github.io, Generalization and Adaptation for Continuous Learning, Open-World Generalization, Introduce sparsely activated LoRA masks and cross-attention-only action experts with a training-free task router to enable mergeable multi-skill VLA policies, BC, LIBERO;LIBERO-Plus;RoboTwin 2.0;SO-101 real robot, LIBERO;LIBERO-Plus;RoboTwin 2.0;SO-101 real robo
GigaWorld-0, 2025-11-30, https://arxiv.org/pdf/2511.19861, https://giga-world-0.github.io, Multi-Modal Fusion and Physical World Representation, Dynamic and Predictive World Models, Unify video generation and 3D physics-aware reconstruction to synthesize controllable embodied interaction data, BC and Predictive modeling and RL, AgiBotWorld;RoboMind;proprietary in-house robotic platforms, PBench Robot Set;DreamGen Bench